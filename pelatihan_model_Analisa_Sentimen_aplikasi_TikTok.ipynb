{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install semua library eksternal dan konfigurasi"
      ],
      "metadata": {
        "id": "72UZTRCjIx9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XiHdLNdp2d6f"
      },
      "outputs": [],
      "source": [
        "# --- Install semua library eksternal ---\n",
        "!pip install gensim --quiet\n",
        "!pip install scikit-learn --quiet\n",
        "!pip install nltk --quiet\n",
        "!pip install matplotlib seaborn --quiet\n",
        "!pip install tensorflow --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy --upgrade --force-reinstall --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBiBu2Zu3YII",
        "outputId": "b6b8295e-1fc6-4cf5-ef5f-524f759acc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix masalah kompatibilitas numpy\n",
        "!pip install numpy==1.23.5 --quiet\n",
        "!pip install gensim --force-reinstall --no-cache-dir --quiet\n",
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW7c6xGi4Qbc",
        "outputId": "f51e6f2d-1784-47ec-ae13-57eb7a4a06e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m206.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m256.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m186.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m163.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m176.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import library"
      ],
      "metadata": {
        "id": "YonYUi-YJLvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D"
      ],
      "metadata": {
        "id": "XRbZdXsdIKH5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 1: Load CSV & Labeling Otomatis"
      ],
      "metadata": {
        "id": "XjkLfYGKRYfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load data hasil scraping\n",
        "df = pd.read_csv('tiktok_reviews.csv')\n",
        "\n",
        "# Bersihkan data kosong\n",
        "df.dropna(subset=['content', 'score'], inplace=True)\n",
        "\n",
        "# Preprocessing dasar: menghapus tanda baca, angka, dan stopwords\n",
        "def clean_text(text):\n",
        "    text = text.lower()                                # huruf kecil\n",
        "    text = re.sub(r'\\d+', '', text)                    # hapus angka\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # hapus tanda baca\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['clean_content'] = df['content'].apply(clean_text)\n",
        "\n",
        "# Labeling berdasarkan analisis polarity (lexicon-based)\n",
        "def get_sentiment(text):\n",
        "    polarity = TextBlob(text).sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        return 'positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['label'] = df['clean_content'].apply(get_sentiment)\n",
        "\n",
        "# Cek distribusi kelas\n",
        "print(df['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZUQJNqN3Hvg",
        "outputId": "2e2a41ce-938a-4e65-d216-6e9ee3178ac6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "positive    56000\n",
            "neutral     38500\n",
            "negative     5500\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 2: Resampling data negative"
      ],
      "metadata": {
        "id": "GDEsFdGxJHvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Pisahkan berdasarkan label\n",
        "df_pos = df[df['label'] == 'positive']\n",
        "df_neu = df[df['label'] == 'neutral']\n",
        "df_neg = df[df['label'] == 'negative']\n",
        "\n",
        "# Cari jumlah maksimum dari semua kelas\n",
        "max_count = max(len(df_pos), len(df_neu), len(df_neg))\n",
        "\n",
        "# Oversample masing-masing kelas ke jumlah maksimum\n",
        "df_pos_upsampled = resample(df_pos, replace=True, n_samples=max_count, random_state=42)\n",
        "df_neu_upsampled = resample(df_neu, replace=True, n_samples=max_count, random_state=42)\n",
        "df_neg_upsampled = resample(df_neg, replace=True, n_samples=max_count, random_state=42)\n",
        "\n",
        "# Gabungkan semua kelas\n",
        "df_balanced = pd.concat([df_pos_upsampled, df_neu_upsampled, df_neg_upsampled])\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42)  # acak ulang\n",
        "\n",
        "# Cek ulang distribusi\n",
        "print(df_balanced['label'].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYmet2tMgs2Q",
        "outputId": "ebe9ee70-94a4-4ee7-f8b0-829aefef3fd5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label\n",
            "neutral     56000\n",
            "positive    56000\n",
            "negative    56000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 3: Encode Label & Split Data"
      ],
      "metadata": {
        "id": "bBicIzWlRslb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Encode label hasil dari lexicon-based analysis pada df_balanced\n",
        "le = LabelEncoder()\n",
        "df_balanced['label_encoded'] = le.fit_transform(df_balanced['label'])  # Contoh: positive=2, neutral=1, negative=0\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df_balanced['clean_content'], df_balanced['label_encoded'],\n",
        "    test_size=0.2,\n",
        "    stratify=df_balanced['label_encoded'],  # agar distribusi kelas tetap proporsional\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training data:\", len(X_train))\n",
        "print(\"Testing data :\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er-cDrLH3qrC",
        "outputId": "248e7ba0-ad55-409d-bab6-16b88bc3a551"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data: 134400\n",
            "Testing data : 33600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 4 : Tokenizing & Padding"
      ],
      "metadata": {
        "id": "Qbhc1KxrRzTI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "MAX_NUM_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Tokenisasi teks hasil preprocessing (clean_content)\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# Ubah teks menjadi urutan angka (sequences)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding agar panjang input seragam\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# One-hot encode label target: 3 kelas (negatif, netral, positif)\n",
        "y_train_cat = to_categorical(y_train, num_classes=3)\n",
        "y_test_cat = to_categorical(y_test, num_classes=3)\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0n48pyk3u1l"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 5 : Word2Vec Embedding Matrix"
      ],
      "metadata": {
        "id": "xQau6sY_R6ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "# Pastikan setiap teks sudah dalam bentuk token list\n",
        "sentences = [text.split() for text in X_train]  # X_train berisi clean_content\n",
        "\n",
        "# Latih model Word2Vec berdasarkan data training\n",
        "w2v_model = Word2Vec(\n",
        "    sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=2,\n",
        "    workers=4,\n",
        "    sg=1  # skip-gram, bisa juga cbow (sg=0) tergantung eksperimen\n",
        ")\n",
        "\n",
        "# Ambil indeks kata dari tokenizer\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Siapkan embedding matrix (untuk layer embedding di model deep learning)\n",
        "embedding_matrix = np.zeros((MAX_NUM_WORDS, 100))\n",
        "for word, i in word_index.items():\n",
        "    if i < MAX_NUM_WORDS:\n",
        "        try:\n",
        "            embedding_vector = w2v_model.wv[word]\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "        except KeyError:\n",
        "            pass  # jika kata tidak ada di model w2v, biarkan tetap 0\n"
      ],
      "metadata": {
        "id": "grU96R-54HM9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 6 : Pelatihan"
      ],
      "metadata": {
        "id": "oZd-GZ7zSOSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skema Pelatihan 1: LSTM + Word2Vec + Split 80/20"
      ],
      "metadata": {
        "id": "vIZE6bSjSMHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(MAX_NUM_WORDS, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model1.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model1.add(Dense(64, activation='relu'))\n",
        "model1.add(Dropout(0.3))\n",
        "model1.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model1.summary()\n",
        "\n",
        "history1 = model1.fit(X_train_pad, y_train_cat, epochs=5, batch_size=512, validation_data=(X_test_pad, y_test_cat))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "cIPF9e_c4zJn",
        "outputId": "b109cec9-e21e-44fe-da9f-5a1cd01b7b66",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 1s/step - accuracy: 0.8640 - loss: 0.3374 - val_accuracy: 0.9854 - val_loss: 0.0406\n",
            "Epoch 2/5\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m336s\u001b[0m 1s/step - accuracy: 0.9845 - loss: 0.0443 - val_accuracy: 0.9854 - val_loss: 0.0385\n",
            "Epoch 3/5\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 1s/step - accuracy: 0.9849 - loss: 0.0417 - val_accuracy: 0.9854 - val_loss: 0.0382\n",
            "Epoch 4/5\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m387s\u001b[0m 1s/step - accuracy: 0.9850 - loss: 0.0407 - val_accuracy: 0.9854 - val_loss: 0.0378\n",
            "Epoch 5/5\n",
            "\u001b[1m263/263\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m329s\u001b[0m 1s/step - accuracy: 0.9847 - loss: 0.0402 - val_accuracy: 0.9854 - val_loss: 0.0376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluasi Skema Pelatihan 1"
      ],
      "metadata": {
        "id": "re6TF8RJSi5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()\n",
        "\n",
        "y_pred1 = model1.predict(X_test_pad)\n",
        "y_pred1_classes = np.argmax(y_pred1, axis=1)\n",
        "\n",
        "print(\"Accuracy Skema 1:\", accuracy_score(y_test, y_pred1_classes))\n",
        "print(classification_report(y_test, y_pred1_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "pOuedR1X-GW-",
        "outputId": "a3da3457-48a9-4f72-82a6-0ae113fe1f55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m117,248\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,377,099\u001b[0m (9.07 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,377,099</span> (9.07 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m125,699\u001b[0m (491.01 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">125,699</span> (491.01 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m251,400\u001b[0m (982.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">251,400</span> (982.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1050/1050\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 44ms/step\n",
            "Accuracy Skema 1: 0.9854464285714286\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     11200\n",
            "     neutral       0.96      1.00      0.98     11200\n",
            "    positive       1.00      0.96      0.98     11200\n",
            "\n",
            "    accuracy                           0.99     33600\n",
            "   macro avg       0.99      0.99      0.99     33600\n",
            "weighted avg       0.99      0.99      0.99     33600\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skema Pelatihan 2: LSTM + TF-IDF + Split 80/20"
      ],
      "metadata": {
        "id": "vE7heH48S-XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf.transform(X_test).toarray()\n",
        "\n",
        "# LSTM but we reshape to 3D\n",
        "X_train_tfidf_3d = np.expand_dims(X_train_tfidf, axis=2)\n",
        "X_test_tfidf_3d = np.expand_dims(X_test_tfidf, axis=2)\n",
        "\n",
        "model2 = Sequential()\n",
        "model2.add(LSTM(128, input_shape=(X_train_tfidf_3d.shape[1], 1)))\n",
        "model2.add(Dense(64, activation='relu'))\n",
        "model2.add(Dropout(0.3))\n",
        "model2.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "history2 = model2.fit(X_train_tfidf_3d, y_train_cat, epochs=5, batch_size=512, validation_data=(X_test_tfidf_3d, y_test_cat))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WKzaZfAKJR",
        "outputId": "ce4f8fb7-7517-4ee0-bff0-5dd0eaa35591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1015s\u001b[0m 6s/step - accuracy: 0.6615 - loss: 0.8690 - val_accuracy: 0.6750 - val_loss: 0.7920\n",
            "Epoch 2/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1020s\u001b[0m 6s/step - accuracy: 0.6754 - loss: 0.7966 - val_accuracy: 0.6750 - val_loss: 0.7895\n",
            "Epoch 3/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1047s\u001b[0m 6s/step - accuracy: 0.6742 - loss: 0.7973 - val_accuracy: 0.6750 - val_loss: 0.7877\n",
            "Epoch 4/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m994s\u001b[0m 6s/step - accuracy: 0.6756 - loss: 0.7900 - val_accuracy: 0.6797 - val_loss: 0.7753\n",
            "Epoch 5/5\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1095s\u001b[0m 7s/step - accuracy: 0.6814 - loss: 0.7830 - val_accuracy: 0.6945 - val_loss: 0.7645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluasi Skema Pelatihan 2"
      ],
      "metadata": {
        "id": "6V6Tk8o-TRVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = model2.predict(X_test_tfidf_3d)\n",
        "y_pred2_classes = np.argmax(y_pred2, axis=1)\n",
        "\n",
        "print(\"Accuracy Skema 2:\", accuracy_score(y_test, y_pred2_classes))\n",
        "print(classification_report(y_test, y_pred2_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCP4lVEcTQKC",
        "outputId": "f6291f46-e5fb-4f90-ef41-ea93c4e88fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 335ms/step\n",
            "Accuracy Skema 2: 0.69455\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.75      0.11      0.20      5200\n",
            "     neutral       0.00      0.00      0.00      1300\n",
            "    positive       0.69      0.99      0.81     13500\n",
            "\n",
            "    accuracy                           0.69     20000\n",
            "   macro avg       0.48      0.37      0.34     20000\n",
            "weighted avg       0.66      0.69      0.60     20000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skema Pelatihan 3: CNN + Word2Vec + Split 70/30"
      ],
      "metadata": {
        "id": "xD4c4XkXTe2T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konstanta\n",
        "MAX_NUM_WORDS = 20000\n",
        "MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "# Split ulang menggunakan data hasil oversampling (df_balanced)\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(\n",
        "    df_balanced['clean_content'], df_balanced['label_encoded'],\n",
        "    test_size=0.3, stratify=df_balanced['label_encoded'], random_state=42\n",
        ")\n",
        "\n",
        "# Tokenisasi ulang menggunakan tokenizer sebelumnya\n",
        "X_train2_seq = tokenizer.texts_to_sequences(X_train2)\n",
        "X_test2_seq = tokenizer.texts_to_sequences(X_test2)\n",
        "\n",
        "# Padding\n",
        "X_train2_pad = pad_sequences(X_train2_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test2_pad = pad_sequences(X_test2_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# One-hot encoding untuk label\n",
        "y_train2_cat = to_categorical(y_train2, num_classes=3)\n",
        "y_test2_cat = to_categorical(y_test2, num_classes=3)\n",
        "\n",
        "# CNN Model\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(MAX_NUM_WORDS, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "model3.add(Conv1D(128, 5, activation='relu'))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(64, activation='relu'))\n",
        "model3.add(Dropout(0.3))\n",
        "model3.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history3 = model3.fit(\n",
        "    X_train2_pad, y_train2_cat,\n",
        "    epochs=5,\n",
        "    batch_size=512,\n",
        "    validation_data=(X_test2_pad, y_test2_cat)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qkb4zo58LId7",
        "outputId": "4ef74d13-bf1e-4eab-ca28-66c08aa74d25"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 421ms/step - accuracy: 0.9049 - loss: 0.2944 - val_accuracy: 0.9855 - val_loss: 0.0393\n",
            "Epoch 2/5\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 410ms/step - accuracy: 0.9859 - loss: 0.0395 - val_accuracy: 0.9855 - val_loss: 0.0378\n",
            "Epoch 3/5\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 384ms/step - accuracy: 0.9849 - loss: 0.0397 - val_accuracy: 0.9855 - val_loss: 0.0372\n",
            "Epoch 4/5\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 417ms/step - accuracy: 0.9849 - loss: 0.0387 - val_accuracy: 0.9855 - val_loss: 0.0359\n",
            "Epoch 5/5\n",
            "\u001b[1m230/230\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 420ms/step - accuracy: 0.9859 - loss: 0.0350 - val_accuracy: 0.9855 - val_loss: 0.0325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluasi Skema Pelatihan 3"
      ],
      "metadata": {
        "id": "Fue6WPZBHRA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tampilkan arsitektur model CNN\n",
        "model3.summary()\n",
        "\n",
        "# Prediksi terhadap data uji\n",
        "y_pred3 = model3.predict(X_test2_pad)\n",
        "y_pred3_classes = np.argmax(y_pred3, axis=1)\n",
        "\n",
        "# True label dari one-hot encoding ke integer\n",
        "y_test3 = np.argmax(y_test2_cat, axis=1)\n",
        "\n",
        "# Akurasi dan Laporan Klasifikasi\n",
        "print(\"Accuracy Skema 3:\", accuracy_score(y_test3, y_pred3_classes))\n",
        "print(classification_report(y_test3, y_pred3_classes, target_names=le.classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "qD65cy9jHApL",
        "outputId": "2c3e5386-783b-433b-9874-c3748ad1d8e7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m96\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m64,128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">96</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">64,128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ global_max_pooling1d            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,217,739\u001b[0m (8.46 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,217,739</span> (8.46 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m72,579\u001b[0m (283.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">72,579</span> (283.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,000,000\u001b[0m (7.63 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> (7.63 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m145,160\u001b[0m (567.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">145,160</span> (567.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1575/1575\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 14ms/step\n",
            "Accuracy Skema 3: 0.9854761904761905\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     16800\n",
            "     neutral       0.96      1.00      0.98     16800\n",
            "    positive       1.00      0.96      0.98     16800\n",
            "\n",
            "    accuracy                           0.99     50400\n",
            "   macro avg       0.99      0.99      0.99     50400\n",
            "weighted avg       0.99      0.99      0.99     50400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langkah 7 Inference Manual (Testing)\n"
      ],
      "metadata": {
        "id": "cS84Q2YTUwUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Model 1"
      ],
      "metadata": {
        "id": "MJ4TexE7HgH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh teks baru\n",
        "new_texts = [\"This app is amazing!\", \"I hate how slow it is\", \"It’s okay, nothing special\"]\n",
        "\n",
        "# 1. Cleaning (gunakan fungsi clean_text sebelumnya)\n",
        "new_texts_cleaned = [clean_text(text) for text in new_texts]\n",
        "\n",
        "# 2. Tokenisasi dan padding\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts_cleaned)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# 3. Prediksi\n",
        "pred_probs = model1.predict(new_padded)\n",
        "pred_classes = pred_probs.argmax(axis=1)\n",
        "\n",
        "# 4. Konversi ke label (gunakan label encoder yang sama)\n",
        "pred_labels = le.inverse_transform(pred_classes)\n",
        "\n",
        "# Tampilkan hasil\n",
        "for text, label in zip(new_texts, pred_labels):\n",
        "    print(f\"Teks: '{text}' → Sentimen: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmE8xCf4vgmA",
        "outputId": "6d4823d4-6bf6-4361-c500-61ccad56354d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 509ms/step\n",
            "Teks: 'This app is amazing!' → Sentimen: positive\n",
            "Teks: 'I hate how slow it is' → Sentimen: negative\n",
            "Teks: 'It’s okay, nothing special' → Sentimen: neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Model 3"
      ],
      "metadata": {
        "id": "jGr0orOVHno2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh teks baru\n",
        "new_texts = [\"I absolutely love this!\", \"I hate this app cause very slow.\", \"It's just average, nothing impressive.\"]\n",
        "\n",
        "\n",
        "# 1. Cleaning (gunakan fungsi clean_text sebelumnya)\n",
        "new_texts_cleaned = [clean_text(text) for text in new_texts]\n",
        "\n",
        "# 2. Tokenisasi dan padding\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts_cleaned)\n",
        "new_padded = pad_sequences(new_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# 3. Prediksi\n",
        "pred_probs = model3.predict(new_padded)\n",
        "pred_classes = pred_probs.argmax(axis=1)\n",
        "\n",
        "# 4. Konversi ke label (gunakan label encoder yang sama)\n",
        "pred_labels = le.inverse_transform(pred_classes)\n",
        "\n",
        "# Tampilkan hasil\n",
        "for text, label in zip(new_texts, pred_labels):\n",
        "    print(f\"Teks: '{text}' → Sentimen: {label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6o9Mo3QMHtO0",
        "outputId": "c3503737-4e05-4a2c-f6df-893f9f0c2964"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "Teks: 'I absolutely love this!' → Sentimen: positive\n",
            "Teks: 'I hate this app cause very slow.' → Sentimen: negative\n",
            "Teks: 'It's just average, nothing impressive.' → Sentimen: neutral\n"
          ]
        }
      ]
    }
  ]
}